{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import brown, wordnet\n",
        "from collections import Counter\n",
        "from nltk import RegexpParser\n",
        "from nltk.tree import Tree"
      ],
      "metadata": {
        "id": "NneByrLFy_zF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "id": "uYyE_RMVzBhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27965db-843d-4b76-baba-63547a91f2b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jargon_df = pd.read_csv(R\"C:\\Users\\ruchi\\Documents\\dataset_1.csv\") #DATASET\n",
        "jargon_text_combined = ' '.join(jargon_df['Original']).lower()#CONVERT TO LOWERCASE\n",
        "\n",
        "jargon_text_cleaned = re.sub(r'[^a-z\\s]', '', jargon_text_combined) #REMOVE OTHER CHARS\n",
        "\n",
        "\n",
        "\n",
        "jargon_word_freqs = Counter(jargon_text_cleaned.split())\n",
        "print(\"✅ ANALYZED 'DATASET_1.CSV' FOR WORD FREQUENCIES.\")"
      ],
      "metadata": {
        "id": "p8ojSH47zF3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74874885-796f-4f5f-818f-5bafe2d73a80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ANALYZED 'DATASET_1.CSV' FOR WORD FREQUENCIES.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "general_word_freqs = Counter(word.lower() for word in brown.words())\n",
        "print(\"✅ GENERAL ENGLISH CORPUS ANALYSIS COMPLETE.\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "E0RdbcFwzRMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7d6f5a-eaec-4aab-b076-893d5e0c50ef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GENERAL ENGLISH CORPUS ANALYSIS COMPLETE.\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_jargon_score(word, jargon_corpus_freq, general_corpus_freq):\n",
        "\n",
        "    freq_in_jargon = jargon_corpus_freq.get(word, 0) + 1 #ADD 1 TO AVOID ZERO DIVISION\n",
        "    freq_in_general = general_corpus_freq.get(word, 0) + 1\n",
        "    jargon_total = sum(jargon_corpus_freq.values())\n",
        "    general_total = sum(general_corpus_freq.values())\n",
        "    score = (freq_in_jargon / jargon_total) / (freq_in_general / general_total)\n",
        "    return score"
      ],
      "metadata": {
        "id": "DpE6XXlSzYLf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_noun_phrase_from_definition(definition):\n",
        "\n",
        "    tokens = nltk.word_tokenize(definition)\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    # RE-CREATING PARSER LOCALLY FOR SELF-CONTAINMENT\n",
        "    grammar_local = r\"\"\" NP: {<DT|JJ|NN.*>*<NN.*>} \"\"\"\n",
        "    cp_local = RegexpParser(grammar_local)\n",
        "    parsed_definition = cp_local.parse(tagged_tokens)\n",
        "\n",
        "    # LOGIC TO SELECT THE MOST RELEVANT NOUN PHRASE (PRIORITIZE THE FIRST ONE)\n",
        "    for chunk in parsed_definition:\n",
        "        if isinstance(chunk, Tree) and chunk.label() == 'NP':\n",
        "\n",
        "\n",
        "\n",
        "            return \" \".join([word for word, tag in chunk.leaves()]) # RETURN THE FIRST NOUN PHRASE FOUND AS A STRING\n",
        "\n",
        "    # IF NO NOUN PHRASE IS FOUND, RETURN THE ORIGINAL DEFINITION\n",
        "    return definition"
      ],
      "metadata": {
        "id": "OCihCMLMza_N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_simplified_meaning(word):\n",
        "    synsets = wordnet.synsets(word)\n",
        "    if synsets:\n",
        "        definition = synsets[0].definition()\n",
        "        if definition:\n",
        "            extracted_np = extract_noun_phrase_from_definition(definition)\n",
        "            if extracted_np:\n",
        "                return extracted_np\n",
        "            else:\n",
        "                return definition\n",
        "\n",
        "        synonyms = set()\n",
        "        for syn in synsets:\n",
        "            for lemma in syn.lemmas():\n",
        "                if lemma.name().lower() != word.lower() and '_' not in lemma.name():\n",
        "                    synonyms.add(lemma.name())\n",
        "        if synonyms:\n",
        "            return list(synonyms)[0]\n",
        "\n",
        "    return word\n",
        "    # This function retrieves a simplified meaning for a given word,\n",
        "    # prioritizing a noun phrase from its WordNet definition or a simple synonym."
      ],
      "metadata": {
        "id": "LFUmIeaPziMf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = r\"\"\"\n",
        "  NP: {<DT|JJ|NN.*>*<NN.*>} # CHUNK DT, JJ, NN.* FOLLOWED BY NN.*\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4Fys20mlzrSt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_phrase_parser = RegexpParser(grammar)\n",
        "\n",
        "\n",
        "def simplify_sentence_with_nltk(sentence):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    parsed_sentence = noun_phrase_parser.parse(tagged_tokens)\n",
        "\n",
        "    simplified_tokens = []\n",
        "    JARGON_SCORE_THRESHOLD = 5.0\n",
        "    JARGON_POS_TAGS = lambda tag: tag.startswith('NN')\n",
        "\n",
        "    for chunk in parsed_sentence:\n",
        "        if isinstance(chunk, Tree) and chunk.label() == 'NP':\n",
        "            is_jargon_np = False\n",
        "            jargon_word_in_np = None\n",
        "            jargon_word_index_in_chunk = -1\n",
        "\n",
        "            np_leaves = chunk.leaves()\n",
        "            for j, (np_word, np_tag) in enumerate(np_leaves):\n",
        "                score = calculate_jargon_score(np_word.lower(), jargon_word_freqs, general_word_freqs)\n",
        "                if score > JARGON_SCORE_THRESHOLD and JARGON_POS_TAGS(np_tag):\n",
        "                    is_jargon_np = True\n",
        "                    jargon_word_in_np = np_word\n",
        "                    jargon_word_index_in_chunk = j\n",
        "                    break\n",
        "\n",
        "            if is_jargon_np and jargon_word_in_np:\n",
        "                simple_meaning_text = get_simplified_meaning(jargon_word_in_np.lower())\n",
        "\n",
        "                modified_np_tokens = []\n",
        "                for i, (np_word, np_tag) in enumerate(np_leaves):\n",
        "                    if i == jargon_word_index_in_chunk:\n",
        "                         modified_np_tokens.append(simple_meaning_text)\n",
        "                    else:\n",
        "                        modified_np_tokens.append(np_word)\n",
        "\n",
        "                simplified_tokens.extend(modified_np_tokens)\n",
        "\n",
        "            else:\n",
        "                simplified_tokens.extend([word for word, tag in chunk.leaves()])\n",
        "        else:\n",
        "            simplified_tokens.append(chunk[0])\n",
        "\n",
        "    simplified_sentence = \"\"\n",
        "    for i, token in enumerate(simplified_tokens):\n",
        "        if simplified_sentence and not token in ('.', ',', '!', '?', ':', ';', ')') and not simplified_sentence.endswith('('):\n",
        "            simplified_sentence += \" \"\n",
        "        simplified_sentence += token\n",
        "\n",
        "    simplified_sentence = re.sub(r'\\s+([?.!,:;])', r'\\1', simplified_sentence)\n",
        "    simplified_sentence = re.sub(r'\\(\\s+', '(', simplified_sentence)\n",
        "    simplified_sentence = re.sub(r'\\s+\\)', ')', simplified_sentence)\n",
        "    simplified_sentence = simplified_sentence.replace('(', '').replace(')', '')\n",
        "\n",
        "\n",
        "    simplified_sentence = simplified_sentence.replace(\" subclass of \", \" \").replace(\" where the usage is restricted to \", \" \").replace(\" that is \", \" \")\n",
        "\n",
        "    return simplified_sentence\n",
        "    # This function simplifies a sentence by identifying and replacing\n",
        "    # jargon within noun phrases using NLTK and WordNet."
      ],
      "metadata": {
        "id": "dlcfQTM-zwyr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GET INPUT FROM USER AND SIMPLIFY ---\n",
        "user_sentence = input(\"ENTER A SENTENCE TO SIMPLIFY: \")\n",
        "# SIMPLIFY THE USER'S SENTENCE\n",
        "simplified_output = simplify_sentence_with_nltk(user_sentence)\n",
        "print(\"\\nSIMPLIFIED SENTENCE:\")\n",
        "# PRINT THE SIMPLIFIED SENTENCE\n",
        "print(simplified_output)"
      ],
      "metadata": {
        "id": "TGgsgXE2z2NK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82278f5d-4aee-4efa-c36a-6bf0b1395f8c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENTER A SENTENCE TO SIMPLIFY: i am a lawyer\n",
            "\n",
            "SIMPLIFIED SENTENCE:\n",
            "i am a a professional person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BLEU SCORE FROM GEMINI -\n"
      ],
      "metadata": {
        "id": "OEBQImvS60PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Assuming jargon_df is already loaded and contains 'Original' and 'Simplified' columns\n",
        "\n",
        "# Prepare data for BLEU score calculation\n",
        "# BLEU score requires the reference to be a list of sentences (even if there's only one reference)\n",
        "# and the hypothesis to be a single sentence.\n",
        "# We will treat 'Original' as the reference and 'Simplified' as the hypothesis.\n",
        "\n",
        "references = []\n",
        "hypotheses = []\n",
        "\n",
        "for index, row in jargon_df.iterrows():\n",
        "    # Check if 'Original' and 'Simplified' are valid strings before processing\n",
        "    original_sentence = row['Original']\n",
        "    simplified_sentence = row['Simplified']\n",
        "\n",
        "    if isinstance(original_sentence, str) and isinstance(simplified_sentence, str):\n",
        "        # Each original sentence is a reference, represented as a list containing one sentence.\n",
        "        references.append([original_sentence.lower().split()]) # Convert to lowercase and split into words\n",
        "        # Each simplified sentence is a hypothesis, as a list of words.\n",
        "        hypotheses.append(simplified_sentence.lower().split()) # Convert to lowercase and split into words\n",
        "    else:\n",
        "        # Optionally, print a message or count skipped rows\n",
        "        print(f\"Skipping row {index} due to invalid data in 'Original' or 'Simplified' columns.\")\n",
        "\n",
        "\n",
        "# Calculate BLEU scores for each sentence pair\n",
        "bleu_scores = []\n",
        "# Using SmoothingFunction() to handle cases where there are no matching n-grams\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "# Ensure there are hypotheses to calculate scores\n",
        "if hypotheses:\n",
        "    for ref, hyp in zip(references, hypotheses):\n",
        "        score = sentence_bleu(ref, hyp, smoothing_function=smoothie)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "    # Calculate the average BLEU score\n",
        "    average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    print(f\"Average BLEU score for the dataset: {average_bleu_score}\")\n",
        "else:\n",
        "    print(\"No valid simplified sentences found to calculate BLEU score.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gyzIZhR6mhu",
        "outputId": "808f7efb-622e-4a83-caae-ab2dc4a3beec"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping row 1135 due to invalid data in 'Original' or 'Simplified' columns.\n",
            "Average BLEU score for the dataset: 0.734554566819249\n"
          ]
        }
      ]
    }
  ]
}
